{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca2bc4bf-8089-4f68-8a8b-4b37eacc1d52",
   "metadata": {},
   "source": [
    "# NER TOKEN CLASSIFICATION\n",
    "\n",
    "Initial steps:\n",
    "- Check CUDA compatibility\n",
    "- Install dependencies\n",
    "- load training data MAKING SURE THE DATA PATH IS CORRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2abf754a-6d87-4c61-aead-ecc37c57ba22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fe5da77-21c6-4391-b9ea-04f9935df142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA T1200 Laptop GPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68eff1d4-e24c-4a9c-a5c5-91cf887378fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "204689db-c523-4a2f-82e6-8eca4c905413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tokens', 'ner_labels'],\n",
       "        num_rows: 218\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tokens', 'ner_labels'],\n",
       "        num_rows: 40\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dfiles= {\"train\": \"./ner_train_split1.jsonl\", \"validation\": \"./ner_validation_split1.jsonl\"}\n",
    "dataset = load_dataset(\"json\", data_files=dfiles)\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "861524b5-fe5f-42e6-94ab-7f20a5bbbda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tokens</th>\n",
       "      <th>ner_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Organic, Cinnamon, Harvest, Cereal]</td>\n",
       "      <td>[B-HLTH, B-FLVR, O, B-PRDT]</td>\n",
       "      <td>[3, 1, 0, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[KitKat, Caramel, Crisp, Wafer, Bar]</td>\n",
       "      <td>[O, B-FLVR, O, B-PRDT, I-PRDT]</td>\n",
       "      <td>[0, 1, 0, 7, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Roasted, Red, Pepper, Alfredo, Spaghetti, Pas...</td>\n",
       "      <td>[B-FLVR, I-FLVR, I-FLVR, B-FLVR, O, B-PRDT, I-...</td>\n",
       "      <td>[1, 2, 2, 1, 0, 7, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Lavender, +, Vanilla, Bean, Deodorant, With, ...</td>\n",
       "      <td>[B-FLVR, O, B-FLVR, I-FLVR, B-PRDT, O, B-FLVR,...</td>\n",
       "      <td>[1, 0, 1, 2, 7, 0, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Garlic, Pasta, Sauce]</td>\n",
       "      <td>[B-FLVR, B-PRDT, I-PRDT]</td>\n",
       "      <td>[1, 7, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>[Naturally, Flavoured, Grape, Soda]</td>\n",
       "      <td>[O, O, B-FLVR, B-PRDT]</td>\n",
       "      <td>[0, 0, 1, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>[Garlic, Spread]</td>\n",
       "      <td>[B-FLVR, B-PRDT]</td>\n",
       "      <td>[1, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>[Thick, &amp;, Juicy, Turkey, Burgers]</td>\n",
       "      <td>[O, O, O, B-PRDT, I-PRDT]</td>\n",
       "      <td>[0, 0, 0, 7, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>[Three, Cheese, Pasta, Sauce]</td>\n",
       "      <td>[O, B-FLVR, B-PRDT, I-PRDT]</td>\n",
       "      <td>[0, 1, 7, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>[LINDOR, 70%, Cacao, Dark, Chocolate, Truffles...</td>\n",
       "      <td>[O, O, O, B-PRDT, I-PRDT, I-PRDT, O]</td>\n",
       "      <td>[0, 0, 0, 7, 8, 8, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>218 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tokens  \\\n",
       "0                 [Organic, Cinnamon, Harvest, Cereal]   \n",
       "1                 [KitKat, Caramel, Crisp, Wafer, Bar]   \n",
       "2    [Roasted, Red, Pepper, Alfredo, Spaghetti, Pas...   \n",
       "3    [Lavender, +, Vanilla, Bean, Deodorant, With, ...   \n",
       "4                               [Garlic, Pasta, Sauce]   \n",
       "..                                                 ...   \n",
       "213                [Naturally, Flavoured, Grape, Soda]   \n",
       "214                                   [Garlic, Spread]   \n",
       "215                 [Thick, &, Juicy, Turkey, Burgers]   \n",
       "216                      [Three, Cheese, Pasta, Sauce]   \n",
       "217  [LINDOR, 70%, Cacao, Dark, Chocolate, Truffles...   \n",
       "\n",
       "                                            ner_tokens  \\\n",
       "0                          [B-HLTH, B-FLVR, O, B-PRDT]   \n",
       "1                       [O, B-FLVR, O, B-PRDT, I-PRDT]   \n",
       "2    [B-FLVR, I-FLVR, I-FLVR, B-FLVR, O, B-PRDT, I-...   \n",
       "3    [B-FLVR, O, B-FLVR, I-FLVR, B-PRDT, O, B-FLVR,...   \n",
       "4                             [B-FLVR, B-PRDT, I-PRDT]   \n",
       "..                                                 ...   \n",
       "213                             [O, O, B-FLVR, B-PRDT]   \n",
       "214                                   [B-FLVR, B-PRDT]   \n",
       "215                          [O, O, O, B-PRDT, I-PRDT]   \n",
       "216                        [O, B-FLVR, B-PRDT, I-PRDT]   \n",
       "217               [O, O, O, B-PRDT, I-PRDT, I-PRDT, O]   \n",
       "\n",
       "                   ner_labels  \n",
       "0                [3, 1, 0, 7]  \n",
       "1             [0, 1, 0, 7, 8]  \n",
       "2       [1, 2, 2, 1, 0, 7, 8]  \n",
       "3    [1, 0, 1, 2, 7, 0, 1, 2]  \n",
       "4                   [1, 7, 8]  \n",
       "..                        ...  \n",
       "213              [0, 0, 1, 7]  \n",
       "214                    [1, 7]  \n",
       "215           [0, 0, 0, 7, 8]  \n",
       "216              [0, 1, 7, 8]  \n",
       "217     [0, 0, 0, 7, 8, 8, 0]  \n",
       "\n",
       "[218 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset['train'].features\n",
    "\n",
    "pd.DataFrame(dataset['train'][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad78908d-76af-4e6f-8c39-952a3b01e1e3",
   "metadata": {},
   "source": [
    "# Load Model and tokenizer from huggingface\n",
    "- Tokenize and align labels\n",
    "- convert tokens to trainable input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da1349ba-0940-4a07-9caa-82f96604bdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint= 'distilbert/distilbert-base-uncased'\n",
    "\n",
    "tokenizer= AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f041144e-531f-4987-bce5-6caaf8f1effe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'lin',\n",
       " '##dor',\n",
       " '70',\n",
       " '%',\n",
       " 'ca',\n",
       " '##cao',\n",
       " 'dark',\n",
       " 'chocolate',\n",
       " 'tr',\n",
       " '##uf',\n",
       " '##fles',\n",
       " 'bag',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_inputs= tokenizer(dataset['train'][217]['tokens'], is_split_into_words=True)\n",
    "\n",
    "temp_inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bfa6a45-82b7-4fb9-a8b6-a3ba817fae41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 0, 1, 1, 2, 2, 3, 4, 5, 5, 5, 6, None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "857c14c1-039f-4de2-8407-d03de7fae6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "  new_labels= []\n",
    "  current_word= None\n",
    "  for word_id in word_ids:\n",
    "    if word_id != current_word:\n",
    "      current_word= word_id\n",
    "      label= -100 if word_id is None else labels[word_id]\n",
    "      new_labels.append(label)\n",
    "    elif word_id == None:\n",
    "      new_labels.append(-100)\n",
    "    else :\n",
    "      label = labels[word_id]\n",
    "      if label%2 ==1 :\n",
    "        label= label+1\n",
    "      new_labels.append(label)\n",
    "\n",
    "  return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1151991e-47d9-40d2-86db-5aade912791b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 7, 8, 8, 0] [None, 0, 0, 1, 1, 2, 2, 3, 4, 5, 5, 5, 6, None]\n"
     ]
    }
   ],
   "source": [
    "#sample data test\n",
    "\n",
    "label_tags= dataset['train'][217]['ner_labels']\n",
    "word_ids= temp_inputs.word_ids()\n",
    "print(label_tags, word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54fd0653-cd04-49d6-969e-492259aa0f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100, 0, 0, 0, 0, 0, 0, 7, 8, 8, 8, 8, 0, -100]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result= align_labels_with_tokens(label_tags, word_ids)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dd10073-f8da-4da5-899c-fecef209ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "  tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "  all_labels = examples['ner_labels']\n",
    "\n",
    "  new_labels = []\n",
    "  for i, labels in enumerate(all_labels):\n",
    "    word_ids = tokenized_inputs.word_ids(i)\n",
    "    new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "  tokenized_inputs['labels'] = new_labels\n",
    "\n",
    "  return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98592d94-9c9d-45ca-a1b3-cfc597b16df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c691144def1c493db106517b40ee5d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_aligned_dataset= dataset.map(tokenize_and_align_labels, batched= True, remove_columns= dataset['train'].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f82eb8fe-2b96-4914-9f5e-b4447f51390d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  11409,\n",
       "  7983,\n",
       "  3963,\n",
       "  1003,\n",
       "  6187,\n",
       "  20808,\n",
       "  2601,\n",
       "  7967,\n",
       "  19817,\n",
       "  16093,\n",
       "  28331,\n",
       "  4524,\n",
       "  102],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [-100, 0, 0, 0, 0, 0, 0, 7, 8, 8, 8, 8, 0, -100]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_aligned_dataset['train'][217]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c0552be-da3f-4898-8510-a004cdc03e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7554, 21229, 11203, 20943,   102,     0,     0,     0,     0],\n",
       "        [  101,  8934, 24498, 14418, 10199, 15594, 11333,  7512,  3347,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[-100,    3,    1,    0,    7, -100, -100, -100, -100, -100],\n",
       "        [-100,    0,    0,    1,    2,    0,    7,    8,    8, -100]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator= DataCollatorForTokenClassification(tokenizer= tokenizer)\n",
    "\n",
    "batch = data_collator([tokenized_aligned_dataset['train'][i] for i in range(2)])\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af36a754-5932-4b57-a175-aa56004ef495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in ./lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in ./lib/python3.10/site-packages (from seqeval) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in ./lib/python3.10/site-packages (from seqeval) (1.4.1.post1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.13.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in ./lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: multiprocess in ./lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: datasets>=2.0.0 in ./lib/python3.10/site-packages (from evaluate) (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in ./lib/python3.10/site-packages (from evaluate) (0.23.4)\n",
      "Requirement already satisfied: pandas in ./lib/python3.10/site-packages (from evaluate) (2.2.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./lib/python3.10/site-packages (from evaluate) (4.66.4)\n",
      "Requirement already satisfied: xxhash in ./lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: dill in ./lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: packaging in ./lib/python3.10/site-packages (from evaluate) (24.0)\n",
      "Requirement already satisfied: responses<0.19 in ./lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in ./lib/python3.10/site-packages (from evaluate) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in ./lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: filelock in ./lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: six>=1.5 in ./lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval\n",
    "!pip install evaluate\n",
    "\n",
    "import evaluate\n",
    "metric = evaluate.load('seqeval')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69341a1-66ab-4c89-8058-82a32d760186",
   "metadata": {},
   "source": [
    "# !IMPORTANT NOTE\n",
    "## !IMPORTANT NOTE\n",
    "### !IMPORTANT NOTE\n",
    "\n",
    "Change the label names if using different labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb94f248-8d9c-4bdc-b408-c8935936c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "label_names= [ \"O\", \n",
    "    \"B-FLVR\",\n",
    "    \"I-FLVR\",\n",
    "    \"B-HLTH\",\n",
    "    \"I-HLTH\",\n",
    "    \"B-MLKF\",\n",
    "    \"I-MLKF\",\n",
    "    \"B-PRDT\",\n",
    "    \"I-PRDT\"\n",
    "]\n",
    "def compute_metrics(eval_preds):\n",
    "  logits, labels = eval_preds\n",
    "\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "  true_labels = [[label_names[l] for l in label if l!=-100] for label in labels]\n",
    "\n",
    "  true_predictions = [[label_names[p] for p,l in zip(prediction, label) if l!=-100]\n",
    "                      for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "  all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "  return {\"precision\": all_metrics['overall_precision'],\n",
    "          \"recall\": all_metrics['overall_recall'],\n",
    "          \"f1\": all_metrics['overall_f1'],\n",
    "          \"accuracy\": all_metrics['overall_accuracy']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de8bc979-d93f-488d-94d1-1d763a96df5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-FLVR', 2: 'I-FLVR', 3: 'B-HLTH', 4: 'I-HLTH', 5: 'B-MLKF', 6: 'I-MLKF', 7: 'B-PRDT', 8: 'I-PRDT'} {'O': 0, 'B-FLVR': 1, 'I-FLVR': 2, 'B-HLTH': 3, 'I-HLTH': 4, 'B-MLKF': 5, 'I-MLKF': 6, 'B-PRDT': 7, 'I-PRDT': 8}\n"
     ]
    }
   ],
   "source": [
    "id2label = {i:label for i, label in enumerate(label_names)}\n",
    "label2id = {label:i for i, label in enumerate(label_names)}\n",
    "\n",
    "print(id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2663211d-69ea-49bf-824b-3b31b63b5464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "                                                    model_checkpoint,\n",
    "                                                    id2label=id2label,\n",
    "                                                    label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c24ebd3-d6cd-4963-9ee6-6630a71cfb24",
   "metadata": {},
   "source": [
    "# BEGIN TRAINING\n",
    "\n",
    "Look for decrease in validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "078947d3-245e-4e80-9552-7edae977ea13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in ./lib/python3.10/site-packages (4.41.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./lib/python3.10/site-packages (from transformers[torch]) (4.66.4)\n",
      "Requirement already satisfied: requests in ./lib/python3.10/site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./lib/python3.10/site-packages (from transformers[torch]) (24.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./lib/python3.10/site-packages (from transformers[torch]) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in ./lib/python3.10/site-packages (from transformers[torch]) (0.23.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: filelock in ./lib/python3.10/site-packages (from transformers[torch]) (3.13.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./lib/python3.10/site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: torch in ./lib/python3.10/site-packages (from transformers[torch]) (2.3.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in ./lib/python3.10/site-packages (from transformers[torch]) (0.31.0)\n",
      "Requirement already satisfied: psutil in ./lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2024.2.0)\n",
      "Requirement already satisfied: triton==2.3.1 in ./lib/python3.10/site-packages (from torch->transformers[torch]) (2.3.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in ./lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.3)\n",
      "Requirement already satisfied: sympy in ./lib/python3.10/site-packages (from torch->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.0.106)\n",
      "Requirement already satisfied: networkx in ./lib/python3.10/site-packages (from torch->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./lib/python3.10/site-packages (from torch->transformers[torch]) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./lib/python3.10/site-packages (from torch->transformers[torch]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./lib/python3.10/site-packages (from torch->transformers[torch]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./lib/python3.10/site-packages (from torch->transformers[torch]) (2.20.5)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./lib/python3.10/site-packages (from torch->transformers[torch]) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./lib/python3.10/site-packages (from requests->transformers[torch]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.10/site-packages (from requests->transformers[torch]) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jg/Documents/bert_models/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140/140 00:14, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.546538</td>\n",
       "      <td>0.229730</td>\n",
       "      <td>0.184783</td>\n",
       "      <td>0.204819</td>\n",
       "      <td>0.495082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.203265</td>\n",
       "      <td>0.266055</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.288557</td>\n",
       "      <td>0.606557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.993956</td>\n",
       "      <td>0.347107</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.394366</td>\n",
       "      <td>0.688525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.898639</td>\n",
       "      <td>0.443478</td>\n",
       "      <td>0.554348</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>0.721311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.872021</td>\n",
       "      <td>0.427350</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.478469</td>\n",
       "      <td>0.718033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jg/Documents/bert_models/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jg/Documents/bert_models/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=140, training_loss=1.140830557686942, metrics={'train_runtime': 15.0652, 'train_samples_per_second': 72.352, 'train_steps_per_second': 9.293, 'total_flos': 4355484410844.0, 'train_loss': 1.140830557686942, 'epoch': 5.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "!pip install transformers[torch]\n",
    "\n",
    "args = TrainingArguments(\"distilbert-finetuned-ner\",\n",
    "                         evaluation_strategy = \"epoch\",\n",
    "                         save_strategy=\"epoch\",\n",
    "                         learning_rate = 2e-5,\n",
    "                         num_train_epochs=5,\n",
    "                         weight_decay=0.01)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset = tokenized_aligned_dataset['train'],\n",
    "                  eval_dataset = tokenized_aligned_dataset['validation'],\n",
    "                  data_collator=data_collator,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  tokenizer=tokenizer)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103679bf-4db1-4c4c-a9fa-998747f3afee",
   "metadata": {},
   "source": [
    "# INFERENCE \n",
    "\n",
    "Checkpoints are automatically saved in local folder as checkpoints.\n",
    "Pick the checkpoints with the lowest loss (each epoch is one checkpoint e.g. epoch 5 is checkpoint 140 [larger number means later checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f894a04a-a26c-4c87-aee4-6b74a65c7482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'FLVR',\n",
       "  'score': 0.41405544,\n",
       "  'word': 'turkey',\n",
       "  'start': 12,\n",
       "  'end': 18},\n",
       " {'entity_group': 'PRDT',\n",
       "  'score': 0.66121984,\n",
       "  'word': 'sausages',\n",
       "  'start': 19,\n",
       "  'end': 27}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "checkpoint = \"./distilbert-finetuned-ner/checkpoint-140\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=checkpoint, aggregation_strategy=\"average\"\n",
    ")\n",
    "\n",
    "result =token_classifier(\"Schneider's Turkey Sausages, 450 g\")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c7d8d-eef1-4767-87e6-0bf6afa07d9b",
   "metadata": {},
   "source": [
    "# Save your checkpoint for use later\n",
    "Check ner_inference Notebook for a standalone example for inferencing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79a690a8-006c-4862-9691-ce2dfe28d9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: distilbert-finetuned-ner/checkpoint-140/ (stored 0%)\n",
      "updating: distilbert-finetuned-ner/checkpoint-140/trainer_state.json (deflated 69%)\n",
      "updating: distilbert-finetuned-ner/checkpoint-140/tokenizer_config.json (deflated 76%)\n",
      "updating: distilbert-finetuned-ner/checkpoint-140/scheduler.pt (deflated 56%)\n",
      "updating: distilbert-finetuned-ner/checkpoint-140/optimizer.pt (deflated 40%)\n",
      "updating: distilbert-finetuned-ner/checkpoint-140/tokenizer.json (deflated 71%)\n",
      "updating: distilbert-finetuned-ner/checkpoint-140/special_tokens_map.json (deflated 42%)\n",
      "updating: distilbert-finetuned-ner/checkpoint-140/vocab.txt (deflated 53%)\n",
      "updating: distilbert-finetuned-ner/checkpoint-140/training_args.bin (deflated 51%)\n",
      "updating: distilbert-finetuned-ner/checkpoint-140/model.safetensors (deflated 8%)\n",
      "updating: distilbert-finetuned-ner/checkpoint-140/rng_state.pth (deflated 25%)\n",
      "updating: distilbert-finetuned-ner/checkpoint-140/config.json (deflated 51%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r ner.zip \"./distilbert-finetuned-ner/checkpoint-140\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a1d4f1-e57e-4adc-9ba6-9fbf0a34d907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
